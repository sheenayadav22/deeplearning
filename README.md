# Deep Learning Project
## Assignment 1: Logistic Regression, Gradient Descent
- Used NumPy to work with arrays, use its various functions, and uses its various mechanisms such as vectorization and Broadcasting.
- Built a complete Logistic Regression system that that can be used to train on data of any number of features.
- Explored qualitative and quantitative techniques for assessing the quality of the trained model.
## Assignment 2: Neural Networks
- Built a complete Neural Network system that that can be used to train on data of any number of features.
- Implemented back propagation without the use of external libraries.
- Explored the abilities of a neural network on non-linearly-separable data.
- Explored how varying the number of hidden units impact accuracy and overfitting.
## Assignment 3: Deep Neural Networks, Multiclass Classification, and Keras
- Built a modular Neural Network framework that supports basic layer types and components.
- Implemented forward propagation and back propagation in a modular and vectorized fashion for common layer types.
- Implemented new functionalities such as ReLU activation, softmax activation, and categorical cross entropy loss.
- Built a multiclass classifier for both structured and unstructured image data.
- Worked with a real dataset (MNIST) and applied training and validation data sets to evaluate a model
- Used Keras to build and train basic neural networks
## Assignment 4: Convolutional Neural Networks
- Extended the deep learning framework to support Convolutional Neural Networks
- Implemented forward propagation and back propagation in a modular fashion for Convolution, Maxpooling, and Flatten layer types.
- Train convolutional neural networks with my custom framework built without the use of external libraries
- Use Keras to build and train convolutional neural networks
- Use Keras to work with mainstream CNN architectures such as ResNet, and VGG
## Assignment 5: Optimization and Regularization
- Extended my deep learning framework to BatchNorm Layers
- Extended my deep learning framework to Dropout Layers
- Implemented forward propagation and back propagation in a modular fashion for BatchNorm and Dropout layers.
- Explored the use of BatchNorm, Dropout, and L2 regularization in training deep neural networks.
