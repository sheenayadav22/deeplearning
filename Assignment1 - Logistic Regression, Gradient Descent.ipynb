{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "58d19d20a0e9bf6e24216039c2974427",
     "grade": false,
     "grade_id": "cell-e0f0209b26d61828",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# CPEN400D Deep Learning\n",
    "\n",
    "Instructors: Brad Quinton, Scott Chin\n",
    "\n",
    "# Assignment 1: Introductions, Logistic Regression, Gradient Descent\n",
    "\n",
    "Welcome to your first assignment!  In this assignment we will introduce the development tools that you will use throughout the course.  Then we will dive in to build a complete Logistic Regression system that you will then use to train on data used to detect defects in chips.\n",
    "\n",
    "After this assignment you will:\n",
    "\n",
    "- Be able to use various practical tools used in Deep Learning implementations such as Python, Jupyter Notebooks, NumPy, matplotlib.\n",
    "- Be able to use NumPy to work with arrays, use its various functions, and uses its various mechanisms such as vectorization and Broadcasting.\n",
    "- Have built a complete Logistic Regression system that can be used to train on data of any number of features!\n",
    "- Have explored some of the considerations when training (e.g. choosing hyperparameters)\n",
    "- Have explored qualitative and quantitative techniques for assessing the quality of your trained model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Completing Your Assignment\n",
    "\n",
    "Your assignment will be graded based on your implementation and completion of specific code cells within this Jupyter Notebook.  We will introduce what Jupyter Notebooks and code cells are in the next section.  \n",
    "\n",
    "The code cells that you need to complete will start with the following text:\n",
    "\n",
    "**\\# GRADED FUNCTION:**\n",
    "\n",
    "Only these cells will be extracted and graded.  Furthermore, within these code cells will be comments \n",
    "\n",
    "**\\### START CODE HERE ###** and **\\### END CODE HERE ###**\n",
    "\n",
    "Write your code **between** these comments!  Do **NOT** change any of the code outside of these comments! In these comments, we will also estimate the number of lines of code that you will need to write. We don't check line count, but if you find yourself going significantly beyond these suggestions, you may consider rethinking your approach.\n",
    "\n",
    "Following each Graded Function code cell will be one or more test cells. You can run these test cells to check that your implementation is correct. We will use these test cells, along with some hidden tests, to grade your assignment.  \n",
    "\n",
    "Submit your assignment via JupyterHub.  You can submit as many times as you'd like.  We will grade the most recently submitted version.\n",
    "\n",
    "Please also edit the following code cell to include your name and student number.  Thank you!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Student Name: Sheena Yadav\n",
    "# Student Number: 49448160"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary of Graded Functions\n",
    "\n",
    "| Function                 | Marks | \n",
    "|--------------------------|-------|\n",
    "| non_vectorized_sigmoid() |1      |\n",
    "| sigmoid()                |1      |\n",
    "| initialize_parameters()  |1      |\n",
    "| hypothesis()             |2      |\n",
    "| compute_cost()           |4      |\n",
    "| compute_gradients()      |4      |\n",
    "| gradient_descent()       |5      |\n",
    "| predict()                |1      |\n",
    "| compute_accuracy()       |1      |\n",
    "| TOTAL                    |20     |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "2068358e55cb5c69a56b8707dc96fa49",
     "grade": false,
     "grade_id": "cell-6916ef18bcaef987",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## 1. Development Environment\n",
    "\n",
    "Let's first review the development environment, programming language, and packages that you will be using throughout this course's assignments.  Some of you may already have experience with some of these components, but it is absolutely ok if you do not. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "4200ea00eaad774da3088b10928ff6df",
     "grade": false,
     "grade_id": "cell-3edcb4ef033eb539",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### 1.1 Python\n",
    "\n",
    "Python is the most commonly used programming language in the field of Deep Learning. The assignments in this course will use Python (version 3).  We will assume you have the following understanding of Python:\n",
    "\n",
    "- Basic datatypes (ints, floats, strings, booleans) and how to manipulate them\n",
    "- Builtin container types (e.g. lists, dicts, sets, tuples) and how to manipulate them (e.g. iteration, indexing, slicing, comprehension)\n",
    "- Basic flow control (loops, conditionals, etc.)\n",
    "- Using and declaring functions\n",
    "- Basic understanding of Classes and how to instantiate and work with objects\n",
    "- Importing packages and modules\n",
    "\n",
    "If you haven't used Python before, don't worry. You will be able to get up to speed very quickly based on the programming experience you have acquired in your engineering program.  There are plenty of good tutorials online. \n",
    "\n",
    "[Official Documentation](https://docs.python.org/3.6/)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "df92bda2eaaff0aadabbcf6a76bb8d31",
     "grade": false,
     "grade_id": "cell-eb10faddfaf1b0ba",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### 1.2. Jupyter Notebooks\n",
    "\n",
    "Jupyter Notebooks (formerly iPython Notebooks) are documents that run in your web browser to produce an interactive computational environment. These notebooks contain *cells* that may contain executable code, or rich media elements (encoded in  [Markdown](https://en.wikipedia.org/wiki/Markdown)) such as text, figures, equations, etc.\n",
    "\n",
    "#### 1.2.1 Running a Cell\n",
    "\n",
    "Executing the code in a cell can be done by pressing SHIFT+ENTER.  Or you can click the *Run* button in the Jupyter menu up top. You can try running the following cell now\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "1333f54fa0cc595aed8e819782d6def3",
     "grade": false,
     "grade_id": "cell-b62378b0a56b7b2c",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello World\n",
      "30\n"
     ]
    }
   ],
   "source": [
    "print(\"Hello World\")\n",
    "\n",
    "a = 10\n",
    "b = 20\n",
    "print(a+b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "1ba3c92c63b5f582365fe522e62c31c4",
     "grade": false,
     "grade_id": "cell-88ed708c6971e84b",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "#### 1.2.2 The Kernel\n",
    "\n",
    "When a Jupyter notebook is opened, its “computational engine” (called the kernel) is automatically started. Each time you run a cell, it is executed by the kernel. You can think of the kernel as sequential program that runs the cells based on the order in which you execute them.  This means you can jump around the notebook and run the cells in a different order than how they are presented.  This also means that variables and functions declared in one executed cell, will reside in the kernel's memory and you can reference it in another cell.  For example, in the previous code cell we declared two Python variables named \"a\" and \"b\".  If you run the following cell, you can see that they are still in the kernel's memory.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "5f0731bacd2e46d4eeac55b3a0ba0e58",
     "grade": false,
     "grade_id": "cell-868ebf3652792b84",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n",
      "20\n"
     ]
    }
   ],
   "source": [
    "print(a)\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "539c3a4f4f1c91748342958533636ea9",
     "grade": false,
     "grade_id": "cell-f1bbee68f5ef7d03",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "To help you keep track of the order in which you executed the cells, you can refer to the text to the left of each code cell which looks like \"In [  ]\".  Once the cell has run, a number will appear in the brackets.  When no number is there, it means the cell has never been run.  If you see an asterisk * in the bracket, it means the cell is currently executing.\n",
    "\n",
    "You can interrupt a running cell by either clicking the square shaped \"Stop\" button in the Jupyter menu above, or going to the \"Kernel\" menu and then selecting \"Interrupt\".\n",
    "\n",
    "Closing the browser does **not** close the kernel. To truly stop it, you need to go to the \"File\" menu and select \"Close and Halt\".  More commonly, you will want to reset your Kernel and to run again from scratch. This doesn't revert or change any of the code in the cells in your notebook, it just simply restarts clears the kernel's memory. To reset the kernel, go to the \"Kernel\" menu, and select one of the restart options.\n",
    "\n",
    "For more info on the kernel, see the [official docs](https://jupyter-notebook-beginner-guide.readthedocs.io/en/latest/execute.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "8e22785e56aa9979dcad268a399a6c31",
     "grade": false,
     "grade_id": "cell-bb89e50581e4b00b",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### 1.3 NumPy\n",
    "\n",
    "NumPy is the core package for scientific computing in Python. It adds support for large, multi-dimensional matrices, along with a large collection of high-level mathematical functions to operate on these arrays efficiently.\n",
    "\n",
    "We do not expect you to have any experience with NumPy. At this point in the assignment, you should familiarize yourself with how to create and manipulate NumPy matrices.  We suggest reviewing the section *The Basics* from the official [NumPy tutorial](https://docs.scipy.org/doc/numpy/user/quickstart.html).  If you have experience with Matlab, you may also want to review this article on [NumPy for Matlab Users](https://docs.scipy.org/doc/numpy/user/numpy-for-matlab-users.html).\n",
    "\n",
    "Run the following code to see some of the basics of working with NumPy arrays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "4d1b1b95cc1b1e2b430a9f0349b86d2c",
     "grade": false,
     "grade_id": "cell-5639769ab90d62b1",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n",
      "(3, 2)\n",
      "5\n"
     ]
    }
   ],
   "source": [
    "# Import the numpy package. It's common practice to import\n",
    "# the packing into an abbreviated identifier called np\n",
    "import numpy as np\n",
    "\n",
    "# Create a 3x2 array\n",
    "x = np.array([(1, 2), \n",
    "              (3, 4), \n",
    "              (5, 6)])\n",
    "\n",
    "# NumPy arrays are objects of a class named ndarray\n",
    "print(type(x))\n",
    "\n",
    "# ndarray objects have a property to inspect the dimensions of the array\n",
    "print(x.shape)\n",
    "\n",
    "# Index into a specific element. Note the use of comma to\n",
    "# delimit the indices.\n",
    "print(x[2, 0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 Matplotlib\n",
    "\n",
    "Matplotlib is a plotting library for Python and NumPy that produces publication-quality figures.  We will be using it to help us visualize our data. This is another package worth learning for industry use.  You can read more about it at the official [Matplotlib website](https://matplotlib.org/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Implementing Logistic Regression\n",
    "\n",
    "The core part of this assignment will be to implement Python code to describe, train, and use for prediction a Logistic Regression model. \n",
    "\n",
    "Recall the equation for the Logistic Regression:\n",
    "\n",
    "$$a = \\sigma(Wx + b)$$\n",
    "\n",
    "- $x$ is the input vector with $n$ features\n",
    "- $w$ is the vector of $n$ learned parameters of the model\n",
    "- $b$ is the learned bias parameter of the model\n",
    "- $\\sigma$ is the Sigmoid activation function\n",
    "- $a$ is the binary prediction output\n",
    "\n",
    "We will step you through each part of this task, and discuss various practical aspects along the way. Let's get started!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Implementing the Sigmoid Function\n",
    "\n",
    "We will start by implementing a Python function to compute the Sigmoid function.  We will use this opportunity to introduce the concept of Vectorization.  \n",
    "\n",
    "A common situation in Deep Learning is that you want to apply the same operation to each element of a vector (or matrix).  The most obvious way to do this is to use a Python loop to iterate over each element of the vector, and then apply the operation to each element. However, this is not efficient.  With modern hardware and software frameworks, it is possible to apply the operation to all elements in the entire vector simultaneously in parallel and/or in a much more optimized way.  This leads to faster execution of the code.  Fast runtime becomes important in Deep Learning as we work with massive datasets, and need to run many iterations of our optimization algorithm (e.g. Gradient Descent) to train our networks. \n",
    "\n",
    "In this section, we will first create a non-vectorized implementation of the Sigmoid function, followed by a vectorized version (which we will ultimately use in our Logistic Regression implementation), and compare their runtimes.\n",
    "\n",
    "First, recall that the Sigmoid function is defined as follows: \n",
    "$$\\sigma(x) = \\frac{1}{1+e^{-x}}$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1.1 Non-vectorized Sigmoid Function\n",
    "\n",
    "Complete the following code to implement a Python function that takes a single int or float, and computes and returns the application of the Sigmoid function.  \n",
    "\n",
    "**Hint**: Use the math.exp() function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "426c9e2bce28220edc0d28f401d40634",
     "grade": false,
     "grade_id": "non_vectorized_sigmoid",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: basic_sigmoid\n",
    "\n",
    "import math\n",
    "\n",
    "def non_vectorized_sigmoid(x):\n",
    "    \"\"\" A non-vectorized implementation of the Sigmoid Function\n",
    "    \n",
    "    Inputs:\n",
    "        x -- A single float or int\n",
    "        \n",
    "    Returns:\n",
    "        A float for the Sigmoid function applied to x\n",
    "    \"\"\"\n",
    "    s = None\n",
    "    ### START CODE HERE ### (~ 1 line of code) ###\n",
    "    \n",
    "    s = 1/(1 + math.exp(-x));\n",
    "    \n",
    "     ### END CODE HERE ###\n",
    "        \n",
    "    return s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the following cell to test your non-vectorized Sigmoid function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "c78486887ed53b8a1541ecfb69863fb8",
     "grade": true,
     "grade_id": "test_non_vectorized_sigmoid",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PASSED: test_non_vectorized_sigmoid()\n"
     ]
    }
   ],
   "source": [
    "def test_non_vectorized_sigmoid():\n",
    "    \"\"\" Testcase for non_vectorized_sigmoid() \"\"\"\n",
    "    x = 1\n",
    "    y_expected = 0.7310585\n",
    "    y = non_vectorized_sigmoid(x)\n",
    "    assert np.allclose(y, y_expected), 'Expected {0} but got {1} for an input of {2}'.format(y_expected, y, x)\n",
    "    \n",
    "    x = 0\n",
    "    y_expected = 0.5\n",
    "    y = non_vectorized_sigmoid(x)\n",
    "    assert np.allclose(y, y_expected), 'Expected {0} but got {1} for an input of {2}'.format(y_expected, y, x)\n",
    "\n",
    "    x = -1\n",
    "    y_expected = 0.2689414\n",
    "    y = non_vectorized_sigmoid(x)\n",
    "    assert np.allclose(y, y_expected), 'Expected {0} but got {1} for an input of {2}'.format(y_expected, y, x)\n",
    "\n",
    "\n",
    "\n",
    "    print('PASSED: test_non_vectorized_sigmoid()')\n",
    "\n",
    "        \n",
    "# Run the test\n",
    "test_non_vectorized_sigmoid()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "0270af3e14e9236595a32e1aa16f2e6b",
     "grade": false,
     "grade_id": "cell-ac156a26004a6b7d",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Try to run the function with a NumPy array as innput. This will not work and will raise an error. See if you can understand how the error traceback points you to the line of offending code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "only size-1 arrays can be converted to Python scalars",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-d49f3a06d231>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnon_vectorized_sigmoid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-5-e28189272e7f>\u001b[0m in \u001b[0;36mnon_vectorized_sigmoid\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0;31m### START CODE HERE ### (~ 1 line of code) ###\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m     \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m;\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m      \u001b[0;31m### END CODE HERE ###\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: only size-1 arrays can be converted to Python scalars"
     ]
    }
   ],
   "source": [
    "x = np.array([1,2,3])\n",
    "print(non_vectorized_sigmoid(x))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1.2 Vectorized Sigmoid function\n",
    "\n",
    "In the last section, we saw that the functions in the *math* library don't support NumPy arrays. Fortunately, the Numpy package provides math operations that do. These functions typically operate on single float and ints as well. To implement a vectorized version of our Sigmoid function, we simply need to use NumPy's version of the *exp()* function.  \n",
    "\n",
    "We won't get into the details of how NumPy's exp() function (and it's other vectorized functions) actually work under-the-hood, but the gist is that there are two things happening.  First, the NumPy functions have low-level optimized pre-compiled implementations (e.g. using the C language) which will be faster than using native-Python loops to iterate across all elements.  Second, these low-level implementations abstract away, and manage hardware resource utilization to make the most out of any parallel processing capabilties of the hardware.  From the developer's (your) perspective, it looks like the operation is applied to all elements at once because you simply pass the exp() function the entire NumPy array, and it returns a NumPy array of the same size with the results of applying exp() to each element. This abstraction also has the benefit of making your code easier to understand.\n",
    "\n",
    "Complete the implementation of the following code to create a vectorized version of the Sigmoid function:\n",
    "\n",
    "**Hint**: Use [np.exp()](https://docs.scipy.org/doc/numpy/reference/generated/numpy.exp.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "63d2e5b50d18ec8b1901f49163f03abe",
     "grade": false,
     "grade_id": "sigmoid",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: sigmoid\n",
    "\n",
    "def sigmoid(x):\n",
    "    \"\"\" A vectorized implementation of the Sigmoid Function\n",
    "    \n",
    "    Inputs:\n",
    "        x -- A NumPy array, or a single float or int\n",
    "        \n",
    "    Returns:\n",
    "        A NumPy array forthe Sigmoid function applied to x\n",
    "    \"\"\"\n",
    "    ### START CODE HERE ### (~ 1 line of code) ###\n",
    "    s = 1/(1 + np.exp(-x));\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    return s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "e0e4f3351e99b364d9900700f9e6cd6d",
     "grade": false,
     "grade_id": "cell-363725bad687bdc0",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Run the following cell to test your vectorized Sigmoid function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "95cd91c919e3057c78080af0a3bac5a8",
     "grade": true,
     "grade_id": "test_sigmoid",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PASSED: test_sigmoid()\n"
     ]
    }
   ],
   "source": [
    "def test_sigmoid():\n",
    "    \"\"\" Testcase for sigmoid() \"\"\"\n",
    "    \n",
    "    x = np.array([1,2,3])\n",
    "    y_expected = np.array([0.73105858, 0.88079708, 0.95257413])\n",
    "    y = sigmoid(x)\n",
    "    assert np.allclose(y, y_expected), 'Expected {0} but got {1} for an input of {2}'.format(y_expected, y, x)\n",
    "\n",
    "\n",
    "    print('PASSED: test_sigmoid()')\n",
    "\n",
    "        \n",
    "# Run the test\n",
    "test_sigmoid()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1.3 Comparing performance of Vectorized implementation\n",
    "\n",
    "Let's compare the runtime of the non-vectorized and vectorized implementations of Sigmoid on a vector with 100,000 elements (this is a small number in terms Deep Learning applications, but is sufficient for demonstration). In the following code, we don't bother saving the results since we just want to measure the runtime.\n",
    "\n",
    "**Note**: It's difficult to get accurate runtime results in this fashion because of other processes currently running on the server (e.g. other students working on their assignment).  But for this example, you should see 10-20x faster runtime for the vectorized implementation.  In other cases of vectorization, you may achieve even greater speedups! Try running this cell several times and you should see the variation in results.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "3ecabfb96f0d4f240d2daf21f83fbfbf",
     "grade": false,
     "grade_id": "cell-0dd0ad61eff7cf5a",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# We will use the time() function to measure runtime\n",
    "import time\n",
    "\n",
    "# Create a random input vector\n",
    "x = np.random.rand(100000)\n",
    "\n",
    "# The Non-vectorized Implementation which requires using\n",
    "# a loop\n",
    "t0 = time.time()\n",
    "for elem in x:\n",
    "    non_vectorized_sigmoid(elem)\n",
    "t1 = time.time()\n",
    "\n",
    "# The Non-vectorized Implementation which accepts the\n",
    "# entire NumPy array\n",
    "sigmoid(x)\n",
    "t2 = time.time()\n",
    "\n",
    "# Now print the runtime results\n",
    "print('Non-vectorized runtime in seconds: {0}'.format(t1-t0))\n",
    "print('Vectorized runtime in seconds:     {0}'.format(t2-t1))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Initializing Parameters\n",
    "\n",
    "Recall that in Logistic Regression, we have a parameter vector $W$ that is equal in size to the number of features $n$, and a single bias parameter, $b$.\n",
    "\n",
    "We will now write a function that initializes these parameters with zeros, and returns them.  Complete the following code. Make sure you create an array that is explicitly sized to be an n-x-1 array for $W$. Note, from here on out, we will use parentheses to denote the dimensions of an array.  So $W$ will be an array with shape (n,1).\n",
    "\n",
    "**Hint**: You may want to use the [np.zeros()](https://docs.scipy.org/doc/numpy/reference/generated/numpy.zeros.html) function. \n",
    "\n",
    "**Notes**: As you work with more complex matrices in Deep Learning implementations, it can be useful to use Python assertions to check the dimensions of the matrices that you create. Common programming mistakes such as accidentally switching the dimension sizes can be caught early this way.  See the assertions already included in the code below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "6d53a9a7f74b03806ba6f313c1e759c6",
     "grade": false,
     "grade_id": "initialize_parameters",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: initialize_parameters\n",
    "\n",
    "def initialize_parameters(n):\n",
    "    \"\"\" Initialize the parameters with zeros for Logistic Regression\n",
    "    \n",
    "    Inputs:\n",
    "        n: An int for number of input features\n",
    "        \n",
    "    Returns:\n",
    "        NumPy Array: the W parameter vector of shape (n, 1)\n",
    "        float: the b bias paramter\n",
    "    \"\"\"\n",
    "   \n",
    "    ### START CODE HERE ### (~ 2 line of code)\n",
    "    w = np.zeros((n,1))\n",
    "    b = float(0)\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    assert(w.shape == (n, 1))\n",
    "    assert(isinstance(b, float))\n",
    "    \n",
    "    return w, b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "0216a7609c7b6271297d0cde063cf64c",
     "grade": true,
     "grade_id": "test_initialize_parameters",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PASSED: test_initialize_parameters()\n"
     ]
    }
   ],
   "source": [
    "def test_initialize_parameters():\n",
    "    \"\"\" Testcase for initialize_parameters() \"\"\"\n",
    "    \n",
    "    n = 6\n",
    "    w_expected = np.array([[0], [0], [0], [0], [0], [0]])\n",
    "    b_expected = 0\n",
    "    w, b = initialize_parameters(n)\n",
    "    \n",
    "    assert np.allclose(w, w_expected), 'For an input of {0}, expected w to be {1}, but got {2}'.format(n, w_expected, w)\n",
    "    assert np.allclose(b, b_expected), 'For an input of {0}, expected b to be {1}, but got {2}'.format(n, b_expected, b)\n",
    "\n",
    "\n",
    "    print('PASSED: test_initialize_parameters()')\n",
    "\n",
    "        \n",
    "# Run the test\n",
    "test_initialize_parameters()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Some NumPy Concepts\n",
    "\n",
    "#### 2.3.1 Vectors in NumPy\n",
    "\n",
    "Strictly speaking, a (1, n) NumPy array, and an (n, 1) NumPy array are both 2-dimensional matrices.  But practically speaking, they can also be regarded as a row vector and a column vector, respectively. A (n, ) NumPy array has only one dimension and is both strictly and practically a vector. This is stumbling block for many people new to NumPy so keep it in mind! It matters when you are trying to perform operations between these two kinds of vectors, and dimensions are importent (e.g. matrix multiply). Furthermore, a NumPy array can have zero dimensions () which is interpreted as a scalar.\n",
    "\n",
    "Here are some examples of these scenarios in code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating some zero vectors of size 10\n",
    "n = 10\n",
    "\n",
    "col_vector = np.zeros((n, 1))\n",
    "row_vector = np.zeros((1, n))\n",
    "vector = np.zeros(n)\n",
    "\n",
    "print('Shape of col_vector: {0}'.format(col_vector.shape))\n",
    "print('Shape of row_vector: {0}'.format(row_vector.shape))\n",
    "print('Shape of vector:     {0}'.format(vector.shape))\n",
    "print()\n",
    "\n",
    "print('Contents of col_vector:')\n",
    "print(col_vector)\n",
    "print()\n",
    "\n",
    "print('Contents of row_vector:')\n",
    "print(row_vector)\n",
    "print()\n",
    "\n",
    "print('Contents of vector - Note the number of brackets compared to row_vector:')\n",
    "print(vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating some vectors from a list\n",
    "n = 10\n",
    "\n",
    "col_vector = np.array([[1], [2], [3], [4]])\n",
    "row_vector = np.array([[1, 2, 3, 4]])\n",
    "vector = np.array([1, 2, 3, 4])\n",
    "\n",
    "print('Shape of col_vector: {0}'.format(col_vector.shape))\n",
    "print('Shape of row_vector: {0}'.format(row_vector.shape))\n",
    "print('Shape of vector:     {0}'.format(vector.shape))\n",
    "print()\n",
    "\n",
    "print('Contents of col_vector:')\n",
    "print(col_vector)\n",
    "print()\n",
    "\n",
    "print('Contents of row_vector:')\n",
    "print(row_vector)\n",
    "print()\n",
    "\n",
    "print('Contents of vector - Note the number of brackets compared to row_vector:')\n",
    "print(vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3.2  NumPy Broadcasting\n",
    "\n",
    "Broadcasting solves the problem of performing matrix arithmetic between matrices of different shapes. According to the [NumPy documentation](https://docs.scipy.org/doc/numpy/user/basics.broadcasting.html): \"Subject to certain constraints, the smaller array is “broadcast” across the larger array so that they have compatible shapes.\"  Broadcasting feels awkward at first for many people just starting to use NumPy, but it is a powerful mechanism that greatly simplifies the written code, making it easier to read and maintain.  Lets start with some specific cases.\n",
    "\n",
    "**Operating on a Scalar and an Array**\n",
    "\n",
    "Say we want to add the same constant $v$ to each element in an array $U$.  Without Broadcasting, one way you could do it is using loops like in the following cell. But we already know that using loops is inefficient, especially when U has many elements. The code gets even more complicated when U is more than 1 dimension as you need nested loops to help iterate over the higher dimensions.  You could use NumPy's [ndarray iterator](https://docs.scipy.org/doc/numpy/reference/arrays.nditer.html) but that is still cumbersome.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "U = np.array([1, 2, 3, 4, 5, 6])\n",
    "v = 10\n",
    "\n",
    "Z = np.empty_like(U)  # Create an empty array with same shape as U\n",
    "for i in range(U.shape[0]):\n",
    "    Z[i] = U[i] + v\n",
    "    \n",
    "print('Result of adding v to each element of U:')\n",
    "print(Z)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another way is to first create a new intermediate array $VV$ with same shape as $U$ and set all elements of $VV$ equal to $V$.  Then you can simply add $U$ and $VV$ together. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "U = np.array([[1, 2, 3],\n",
    "              [4, 5, 6]])\n",
    "v = 10\n",
    "\n",
    "# Replicate v into a new array VV \n",
    "VV = np.tile(v, (U.shape))\n",
    "Z = U + VV\n",
    "\n",
    "print('VV:')\n",
    "print(VV)\n",
    "print()\n",
    "print('Result of adding v to each element of U:')\n",
    "print(Z)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok that feels more like the vectorized NumPy way. NumPy goes one step further by basically doing the above for you via Broadcasting. You can simply write \"U + v\", and NumPy will implicitly handle the above for you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "U = np.array([[1, 2, 3],\n",
    "              [4, 5, 6]])\n",
    "v = 10\n",
    "\n",
    "Z = U + v\n",
    "\n",
    "print('Result of Broadcasting on U+v:')\n",
    "print(Z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The advantage of using Broadcasting are two-fold:\n",
    "\n",
    "- 1. Simplifies code (which is important to keep code easy to maintain, test, and extend!)\n",
    "- 2. The creation of the intermediate arrays are done in a more computationally efficient way than if you did it yourself.  Under the hood, it doesn't actually allocate and populate a new array in memory. The necessary arithmetic operation (e.g. $+v$ in the previous examples) is simply *broadcast* to each element of the larger operand (e.g. $U$ in the previous examples). By skipping the intermediate array allocation and population, Broadcasting is both more memory efficient and processing efficient."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Operating on Arrays of Different Size**\n",
    "\n",
    "Broadcasting also works between arrays of different size. Consider the following example where $U$ is a 2D matrix, and $V$ is a row vector. Now say we want to add $V$ to each row of $U$.  You could manually stack copies of $V$ into an intermediate array with same number of rows as $U$ and perform the addition like in the next cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "U = np.array([[1, 2, 3],\n",
    "              [4, 5, 6],\n",
    "              [7, 8, 9]])\n",
    "V = np.array([3, 4, 5])\n",
    "\n",
    "# Replicate v into a new array VV \n",
    "VV = np.tile(V, (U.shape[0], 1))\n",
    "Z = U + VV\n",
    "\n",
    "print('VV:')\n",
    "print(VV)\n",
    "print()\n",
    "print('Result of adding V to each row of U:')\n",
    "print(Z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Broadcasting can handle this for you:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "U = np.array([[1, 2, 3],\n",
    "              [4, 5, 6],\n",
    "              [7, 8, 9]])\n",
    "V = np.array([3, 4, 5])\n",
    "\n",
    "Z = U + V\n",
    "\n",
    "print('Result of Broadcasting and U+V:')\n",
    "print(Z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Broadcasting Compatibility Rules**\n",
    "\n",
    "A set of rules govern whether two arrays are compatible for Broadcasting. To determine compatibility, the following two steps are taken\n",
    "\n",
    "1. If the two arrays have a differing number of dimensions, prepend the shape of the array with the smaller number of dimensions with dimensions of size 1 until both arrays have the same number of dimensions.\n",
    "\n",
    "**Example**\n",
    "$U$ is shape (3, 2, 4) and V is of shape (4,).  To perform compatibility check, $V$ is padded to (1, 1, 4)\n",
    "\n",
    "**Example**\n",
    "$U$ is shape (2, 3) and V  is of shape ().  To perform compatibility check, $V$ is padded out to (1, 1)\n",
    "\n",
    "We can prepend dimensions of size 1 because this doesn't change the contents of the array. Can you convince yourself why? Think about a scalar, and then think about considering it as an array of shape (1,1,1)\n",
    "\n",
    "2. The size of each dimension are then compared.  The dimension is deemed compatible under two conditions:\n",
    "  1. Both dimensions are of the same size\n",
    "  2. One of the dimensions is size 1\n",
    "  \n",
    "**Example**\n",
    "$U$ is shape (3, 2, 4) and $V$ is (1, 1, 4)\n",
    "  - Dimension 0: Compare 3 and 1. This dimension is compatible because one of them is size 1\n",
    "  - Dimension 1: Compare 2 and 1. This dimension is compatible because one of them is size 1\n",
    "  - Dimension 2: Compare 4 and 4. This dimension is compatible because they are both of the same size.\n",
    "\n",
    "**Example**\n",
    "$U$ is shape (2, 8) and $V$ is (2, 6)\n",
    "  - Dimension 0: Compare 2 and 2. This dimension is compatible because they are both of the same size.\n",
    "  - Dimension 1: Compare 8 and 6. This dimension is **not** compatible\n",
    "\n",
    "\n",
    "3. When all dimensions are compatible, the two arrays are deemed compatible for Broadcasting.\n",
    "\n",
    "In code, when you try to apply broadcasting but the two arrays are not compatible, the Python interpreter will throw an error. \"ValueError: operands could not be broadcast together\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What Happens During Broadcasting**\n",
    "\n",
    "To perform the Broadcasting operation, both input arrays are treated as follows. For each case where a dimension is size 1 for one array and greater than 1 for the other, copy the array that has dimension size 1 along this dimension n times where n is the size of the dimension that is greater than 1.\n",
    "\n",
    "After this process, the shape of the resulting arrays is such that each dimension is the maximum of that dimension among the two original arrays.\n",
    "\n",
    "**Example**\n",
    "$U$ is shape (2, 3) and $V$ is (1, 3). As follows:\n",
    "\n",
    "\n",
    "$$ U = \\begin{bmatrix}\n",
    "    1 & 2 & 3 \\\\\n",
    "    4 & 5 & 6 \\\\\n",
    "\\end{bmatrix}, V = \\begin{bmatrix}\n",
    "    7 & 8 & 9 \\\\\n",
    "\\end{bmatrix}$$ \n",
    "\n",
    "In dimension 0, $V$ is size 1 whereas $U$ is size 2. So $V$ is copied along this dimension until it is size 2.\n",
    "V now has a shape of (2, 3)\n",
    "\n",
    "$$ V = \\begin{bmatrix}\n",
    "    7 & 8 & 9 \\\\\n",
    "    7 & 8 & 9 \\\\\n",
    "\\end{bmatrix}$$ \n",
    "\n",
    "**Example**\n",
    "\n",
    "$U$ is shape (1, 3) and $V$ is a vector of shape (4, 1) as follows:\n",
    "\n",
    "$$ U = \\begin{bmatrix}\n",
    "    1 & 2 & 3 \n",
    "\\end{bmatrix}, V = \\begin{bmatrix}\n",
    "    4 \\\\ 5 \\\\ 6 \\\\ 7 \\\\\n",
    "\\end{bmatrix}$$  \n",
    "\n",
    "In dimension 0, $U$ is size 1 and $V$ is size 4. So $U$ is copied 4 times along this dimension. In dimension 1, $V$ is size 1 and $U$ is size 3, so $V$ is copied out 3 times along this dimension to yield the following two effective arrays after Broadcasting. Both of these arrays have a shape of (4, 3):\n",
    "\n",
    "$$ U = \\begin{bmatrix}\n",
    "    1 & 2 & 3 \\\\\n",
    "    1 & 2 & 3 \\\\\n",
    "    1 & 2 & 3 \\\\\n",
    "    1 & 2 & 3 \\\\\n",
    "\\end{bmatrix}, V = \\begin{bmatrix}\n",
    "    4 & 4 & 4 \\\\\n",
    "    5 & 5 & 5 \\\\\n",
    "    6 & 6 & 6 \\\\\n",
    "    7 & 7 & 7 \\\\\n",
    "\\end{bmatrix}$$\n",
    "\n",
    "Try this one out in the next cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "U = np.array([[1, 2, 3]])\n",
    "V = np.array([[4], [5], [6], [7]])\n",
    "U + V"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We glossed over this at the time, but when you wrote your vectorized sigmoid function, you would have used Broadcasting.  Go back and have a look. Can you see that this is true?\n",
    "\n",
    "Operations that support Broadcasting are called [Universal Functions](https://docs.scipy.org/doc/numpy/reference/ufuncs.html).\n",
    "\n",
    "For more info on Broadcasting, you can read the [official docs](https://docs.scipy.org/doc/numpy/user/basics.broadcasting.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Implementing the Hypothesis Function\n",
    "\n",
    "Recall from lecture the vectorized Logistic Regression hypothesis function:\n",
    "\n",
    "$A = \\sigma(w^T X + b)$\n",
    "\n",
    "- $X$ is the (n, m) input matrix where $n$ is the number of features, and $m$ is the number of data samples.\n",
    "- $w$ is the (n, 1) parameter matrix.\n",
    "- $b$ is the bias parameter.   \n",
    "- $A$ is a (1, m) vector which represents the hypothesis for each of the $m$ samples.  \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now build a Python function to implement the **vectorized** Logistic Regression hypothesis function.  In this case, **vectorized** refers to computing the hypothesis for all of the $m$ data samples simultaneously as supplied via the array $X$.\n",
    "\n",
    "First, consider the hypothesis for one sample $x^{(i)}$: \n",
    "\n",
    "$$a_i = \\sigma(w_0 x^{(i)}_0 + w_1x^{(i)}_1 + ... + w_nx^{(i)}_n + b)$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then expand the (1,m) matrix $A$ as follows:\n",
    "\n",
    "$$ A = \\begin{bmatrix} \n",
    "    a_1 & a_1 & \\dots & a_m  \\\\\n",
    "\\end{bmatrix} =\n",
    "\\begin{bmatrix}\n",
    "    \\sigma(w_0 x^{(0)}_0 + \\dots + w_nx^{(0)}_n + b)  &\n",
    "    \\sigma(w_0 x^{(1)}_0 + \\dots + w_nx^{(1)}_n + b)  &\n",
    "    \\dots &\n",
    "    \\sigma(w_0 x^{(m)}_0 + \\dots + w_nx^{(m)}_n + b)\n",
    "\\end{bmatrix}$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We already wrote a vectorized implementation of the Sigmoid function which can operate on each element of a matrix.  Furthermore, we also discussed about Broadcasting, so we will be able to use it for adding $b$. The only part remaining then is how to compute the inner matrix in a vectorized manner?\n",
    "\n",
    "$$ A = \\sigma\\left(\\begin{bmatrix}\n",
    "    w_0 x^{(0)}_0 + \\dots + w_nx^{(0)}_n  &\n",
    "    w_0 x^{(1)}_0 + \\dots + w_nx^{(1)}_n  &\n",
    "    \\dots &\n",
    "    w_0 x^{(m)}_0 + \\dots + w_nx^{(m)}_n \n",
    "\\end{bmatrix} + b\\right) $$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From Linear Algebra, you should be able to convince yourselves that $w^TX$ gives us the inner matrix that we need in the previous line:\n",
    "\n",
    "$$ w^TX = \\begin{bmatrix}\n",
    "    w_0 & w_1 & \\dots & w_n \\\\\n",
    "\\end{bmatrix} \\begin{bmatrix}\n",
    "    x^{(0)}_0 & x^{(1)}_0 & \\dots & x^{(m)}_0 \\\\\n",
    "    x^{(0)}_1 & x^{(1)}_1 & \\dots & x^{(m)}_1 \\\\\n",
    "    \\vdots    & \\ddots  \\\\\n",
    "    x^{(0)}_n & x^{(1)}_n & \\dots & x^{(m)}_n \\\\\n",
    "\\end{bmatrix} = \\begin{bmatrix}\n",
    "    w_0 x^{(0)}_0 + \\dots + w_nx^{(0)}_n  &\n",
    "    w_0 x^{(1)}_0 + \\dots + w_nx^{(1)}_n  &\n",
    "    \\dots &\n",
    "    w_0 x^{(m)}_0 + \\dots + w_nx^{(m)}_n \n",
    "\\end{bmatrix} $$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So to recap, we will \n",
    "- use vectorized NumPy operations to compute $W^TX$\n",
    "- use Broadcasting to add $b$ to the above in a vectorized manner\n",
    "- use your vectorized Sigmoid function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now complete the following code to implement the vectorized hypothesis function.\n",
    "\n",
    "**Hint**: \n",
    "- To get the transpose of a matrix, simply use the .T property of a NumPy array\n",
    "- NumPy matrix multiply can be peformed by [np.matmul()](https://docs.scipy.org/doc/numpy/reference/generated/numpy.matmul.html)\n",
    "- Make use of your vectorized Sigmoid function that you implemented in Section 2.1.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "15360fef9e0dc55bde03faa8cbadfe37",
     "grade": false,
     "grade_id": "hypothesis",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: hypothesis\n",
    "\n",
    "def hypothesis(X, w, b):\n",
    "    \"\"\" \n",
    "    Inputs:\n",
    "        X: NumPy array of input samples of shape (n, m)\n",
    "        w: NumPy array of parameters with shape (n, 1)\n",
    "        b: float for the bias parameter\n",
    "        \n",
    "    Returns:\n",
    "        NumPy array of shape (1, m) with the hypothesis of each sample\n",
    "    \"\"\"\n",
    "    A = None\n",
    "    \n",
    "    ### START CODE HERE ### (~ 1 line of code)\n",
    "    A = sigmoid(np.matmul(w.T,X) + b)\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    return A\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "687e5b623886a9690411e9da57a64f64",
     "grade": true,
     "grade_id": "test_hypothesis",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PASSED: test_hypothesis()\n"
     ]
    }
   ],
   "source": [
    "def test_hypothesis():\n",
    "    \"\"\" Testcase for hypothesis() \"\"\"\n",
    "    \n",
    "    X = np.array([[1, 0.5, 0.23], \n",
    "                  [0.95, 0.43, 0.14], \n",
    "                  [0.78, 0.33, 0.31]])\n",
    "    w = np.array([[1.55], [0.25], [0.1]])\n",
    "    b = 0.13\n",
    "    \n",
    "    A_expected = np.array([[0.8803238, 0.73990984, 0.63471542]])\n",
    "    A = hypothesis(X, w, b)\n",
    "    \n",
    "    assert isinstance(A, np.ndarray), 'Expected a Numpy array for A but got {0}'.format(type(A))\n",
    "    assert A.shape == A_expected.shape, 'Unexpected shape for A. Expected {0} but got {1}'.format(\n",
    "        A_expected.shape, A.shape)\n",
    "    assert np.allclose(A, A_expected), 'expected A to be {0}, but got {1}'.format(A_expected, A)\n",
    "\n",
    "    \n",
    "\n",
    "    print('PASSED: test_hypothesis()')\n",
    "\n",
    "        \n",
    "# Run the test\n",
    "test_hypothesis()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Computing the Cost\n",
    "\n",
    "Recall the Logistic Regression cost function as follows:\n",
    "\n",
    "$$J = -\\frac{1}{m}\\sum_{i=1}^{m}y^{(i)}\\log(a^{(i)})+(1-y^{(i)})\\log(1-a^{(i)})$$\n",
    "\n",
    "Complete the following code to compute the above cost using a vectorized implementation (don't use loops!).  The Python function will accept $A$ as an array of shape (1, m) and $Y$ as a vector of shape (m,).\n",
    "\n",
    "**Hints**\n",
    "\n",
    "- Consider the cost function with the summation split into two summations as follows: \n",
    "\n",
    "$$J = -\\frac{1}{m}\\left(\\sum_{i=1}^{m}y^{(i)}\\log(a^{(i)})+\\sum_{i=1}^{m}(1-y^{(i)})\\log(1-a^{(i)})\\right)$$\n",
    "\n",
    "- Figure out how to write vectorized code to do the first summation: $\\sum_{i=1}^{m}y^{(i)}\\log(a^{(i)})$\n",
    "- This is an elementwise multiplication between two vectors, followed by a summation. You can do both of this in one step using the appropriate linear algebra operation, but you can also do it separately.\n",
    "- There are a couple opportunities to apply Broadcasting.\n",
    "- You may want to use one or more of [np.log()](https://docs.scipy.org/doc/numpy/reference/generated/numpy.log.html), [np.sum()](https://docs.scipy.org/doc/numpy/reference/generated/numpy.sum.html)\n",
    "- Remember that you need to return a single float. You can cast a NumPy array with a single element to a Python float using float()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "c7938cdb83ebf5a56354f89e28f6b334",
     "grade": false,
     "grade_id": "compute_cost",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: compute_cost\n",
    "\n",
    "def compute_cost(A, Y):\n",
    "    \"\"\" Vectorized Logistic Regression Cost Function\n",
    "    \n",
    "    Inputs:\n",
    "        A: NumPy array of shape (1, m)\n",
    "        Y: NumPy array (m, ) of known labels\n",
    "        \n",
    "    Returns:\n",
    "        A single float for the cost\n",
    "    \"\"\"\n",
    "    \n",
    "    cost = None\n",
    "    \n",
    "    ### START CODE HERE ### (~ 1-5 line of code)\n",
    "    cost = float(-(np.matmul(np.log(A),Y) + np.matmul(np.log(1-A),1-Y))/Y.size)\n",
    "    ### END CODE HERE ###\n",
    "    return cost\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "ce5b34ee0d35326f2aa35397d7595101",
     "grade": true,
     "grade_id": "test_compute_cost",
     "locked": true,
     "points": 4,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PASSED: test_compute_cost()\n"
     ]
    }
   ],
   "source": [
    "def test_compute_cost():\n",
    "    \"\"\" Testcase for compute_cost() \"\"\"\n",
    "    \n",
    "    A = np.array([[0.1, 0.5, 0.75]])\n",
    "    Y = np.array([1, 0, 1])\n",
    "    \n",
    "    cost_expected = 1.09447145\n",
    "    cost = compute_cost(A, Y)\n",
    "    \n",
    "    assert isinstance(cost, float), 'Expected a float for cost but got {0}'.format(type(cost))\n",
    "    assert np.allclose(cost, cost_expected), 'expected cost to be {0}, but got {1}'.format(cost_expected, cost)\n",
    "    \n",
    "\n",
    "    print('PASSED: test_compute_cost()')\n",
    "\n",
    "        \n",
    "# Run the test\n",
    "test_compute_cost()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5 Computing Gradients\n",
    "\n",
    "To perform gradient descent, we need to compute the derivative of the cost function $J$ with respect to each parameter. Recall from lecture the following two equations for the gradients with respect to $w$ and $b$:\n",
    "\n",
    "$$ \\frac{\\partial J}{\\partial w} = \\frac{1}{m}X(A-Y)^T$$\n",
    "$$ \\frac{\\partial J}{\\partial b} = \\frac{1}{m} \\sum_{i=1}^m (a^{(i)}-y^{(i)})$$\n",
    "\n",
    "Now complete the following Python function to compute these two derivatives.\n",
    "\n",
    "**Hint**: You may find the [NumPy sum()](https://docs.scipy.org/doc/numpy/reference/generated/numpy.sum.html) function to be useful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "88e3a4e341bbc6ecb2678b58ba6ca78e",
     "grade": false,
     "grade_id": "compute_gradients",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: compute_gradients\n",
    "\n",
    "def compute_gradients(A, X, Y):\n",
    "    \"\"\" Compute the gradients of the cost function \n",
    "    \n",
    "    Inputs:\n",
    "        A: NumPy array of shape (1, m)\n",
    "        X: NumPy array of shape (n, m)\n",
    "        Y: NumPy array of shape (m, )\n",
    "    \n",
    "    Returns:\n",
    "        Two NumPy arrays. One for the cost derivative w.r.t. dw\n",
    "        and one for the cost derivative w.r.t. db\n",
    "    \"\"\"\n",
    "    dw = None\n",
    "    db = None\n",
    "    \n",
    "    ### START CODE HERE ### (~ 2-3 line of code)\n",
    "    dw = np.matmul(X,(A-Y).T)/Y.size\n",
    "    db = (np.sum(A) - np.sum(Y))/Y.size\n",
    "    ### END CODE HERE ###\n",
    "\n",
    "    assert(dw.shape == (X.shape[0], 1))\n",
    "\n",
    "    return dw, db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "e001aa848891bd0fa33c0fa8fa3a6184",
     "grade": true,
     "grade_id": "test_compute_gradients",
     "locked": true,
     "points": 4,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PASSED: test_compute_gradients()\n"
     ]
    }
   ],
   "source": [
    "def test_compute_gradients():\n",
    "    \"\"\" Testcase for compute_gradients() \"\"\"\n",
    "    \n",
    "    A = np.array([[0.99, 0.21, 0.87]])\n",
    "    X = np.array([[1, 0.5, 0.23], \n",
    "                  [0.95, 0.43, 0.14], \n",
    "                  [0.78, 0.33, 0.31]])\n",
    "    Y = np.array([[1, 0, 1]])\n",
    "    \n",
    "    dw_expected = np.array([[0.0217    ],\n",
    "                            [0.02086667],\n",
    "                            [0.00706667]])\n",
    "    db_expected = 0.0233333333\n",
    "    dw, db = compute_gradients(A, X, Y)\n",
    "\n",
    "    assert isinstance(dw, np.ndarray), 'Expected a Numpy array for dw but got {0}'.format(type(dw))\n",
    "    assert dw.shape == dw_expected.shape, 'Unexpected shape for dw. Expected {0} but got {1}'.format(\n",
    "        dw_expected.shape, dw.shape)\n",
    "    assert np.allclose(dw, dw_expected), 'expected dw to be {0}, but got {1}'.format(dw_expected, dw)\n",
    "\n",
    "    assert isinstance(db, float), 'Expected a float for db but got {0}'.format(type(db))\n",
    "    assert np.allclose(db, db_expected), 'expected db to be {0}, but got {1}'.format(db_expected, db)\n",
    "    \n",
    "    \n",
    "\n",
    "    print('PASSED: test_compute_gradients()')\n",
    "\n",
    "        \n",
    "# Run the test\n",
    "test_compute_gradients()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.6 Gradient Descent\n",
    "\n",
    "To train our logistic regression model, we will now put all the previous parts together to implement Gradient Descent. Recall the Gradient Descent algorithm:\n",
    "\n",
    "- initialize trainable parameters\n",
    "- for num_iterations:\n",
    "  - compute gradients when making hypothesis with current parameters\n",
    "  - update parameters according to a learning rate\n",
    "  \n",
    "Although we do not need to compute the cost within the loop, we typically do it to help us see how Gradient Descent is progressing.\n",
    "\n",
    "**Hints**\n",
    "\n",
    "- Remember that you already wrote the following functions: *initialize_parameters()*, *hypothesis()*, *compute_cost()*, *compute_gradients()*. Use them here!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "87571b5b13632a38446a03028f49dc2f",
     "grade": false,
     "grade_id": "gradient_descent",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: gradient_descent\n",
    "\n",
    "def gradient_descent(X, Y, num_iterations, learning_rate, print_costs=True):\n",
    "    \"\"\" Perform Gradient Descent for Logistic Regression\n",
    "    \n",
    "    Inputs:\n",
    "        X: NumPy array (n, m)\n",
    "        Y: NumPy array (m,)\n",
    "        num_iterations: int for number of gradient descent iterations\n",
    "        learning_rate: float for gradient descent learning rate\n",
    "        print_costs: bool to enable printing of costs\n",
    "    \n",
    "    Returns:\n",
    "        w: NumPy array for trained parameters w\n",
    "        b: float for trained bias parameter b\n",
    "        costs: Python list of cost at each iteration\n",
    "    \"\"\"\n",
    "    w, b = None, None\n",
    "\n",
    "    ### START CODE HERE ### (~1  line of code)\n",
    "    w,b = initialize_parameters(X.shape[0])\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    # We will use a list to store the cost at each iteration\n",
    "    # so that we can plot this later for educational purposes\n",
    "    costs = []\n",
    "    \n",
    "    for i in range(num_iterations):\n",
    "\n",
    "        ### START CODE HERE ### (~5  line of code)\n",
    "        A = hypothesis(X, w, b)\n",
    "        dw, db = compute_gradients(A, X, Y)\n",
    "        cost = compute_cost(A, Y)\n",
    "        w = w - learning_rate*dw\n",
    "        b = b - learning_rate*db\n",
    "        ### END CODE HERE ###\n",
    "        \n",
    "        # Convert and save the cost at this iteration\n",
    "        costs.append(cost)\n",
    "        \n",
    "        # Print cost after ever 5000 iterations\n",
    "        if print_costs and i % 5000 == 0:\n",
    "            print(\"Iteration {0} - Cost: {1}\".format(i, str(costs[-1])))\n",
    "\n",
    "    return w, b, costs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "f2d38b9cb3b335f04f90a74ce1201c5f",
     "grade": true,
     "grade_id": "test_gradient_descent_one_iteration",
     "locked": true,
     "points": 4,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PASSED: test_gradient_descent_one_iteration()\n"
     ]
    }
   ],
   "source": [
    "def test_gradient_descent_one_iteration():\n",
    "    \"\"\" Testcase for gradient_descent for one iteration() \"\"\"\n",
    "    \n",
    "    X = np.array([[71.99, 57.95, 73.10, 63.45, 82.74, 18.05, 80.31, 3.76, 66.02, 42.84],\n",
    "                   [67.21, 1.41, 30.45, 10.01, 97.86, 6.09, 52.75, 19.03, 80.13, 48.92]])\n",
    "    Y = np.array([0, 0, 0, 1, 0, 1, 0, 1, 0, 1])\n",
    "    num_iterations = 1\n",
    "    learning_rate = 0.001182\n",
    "    \n",
    "    w_expected = np.array([[-0.01796699],\n",
    "                           [-0.01452442]])\n",
    "    b_expected = -0.0001182\n",
    "    costs_expected = [0.69314718,]\n",
    "    \n",
    "    w, b, costs = gradient_descent(X, Y, num_iterations, learning_rate, print_costs=False)\n",
    "\n",
    "    assert isinstance(w, np.ndarray), 'Expected a Numpy array for w but got {0}'.format(type(w))\n",
    "    assert w.shape == w_expected.shape, 'Unexpected shape for w. Expected {0} but got {1}'.format(\n",
    "        w_expected.shape, w.shape)\n",
    "    assert np.allclose(w, w_expected), 'expected w to be {0}, but got {1}'.format(w_expected, w)\n",
    "\n",
    "    assert isinstance(b, float), 'Expected a float for b but got {0}'.format(type(b))\n",
    "    assert np.allclose(b, b_expected), 'expected b to be {0}, but got {1}'.format(b_expected, b)\n",
    "    \n",
    "    assert isinstance(costs, list), 'Expected a Python list for costs but got {0}'.format(type(costs))\n",
    "    assert len(costs) == len(costs_expected), 'Unexpected length for costs. Expected {0} but got {1}'.format(\n",
    "        len(costs_expected), len(costs))\n",
    "    assert np.allclose(costs, costs_expected), 'expected costs to be {0}, but got {1}'.format(costs_expected, costs)\n",
    "\n",
    "\n",
    "    print('PASSED: test_gradient_descent_one_iteration()')\n",
    "\n",
    "        \n",
    "# Run the test\n",
    "test_gradient_descent_one_iteration()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "7f1cf09faf720ec50dde7ea710742b48",
     "grade": true,
     "grade_id": "test_gradient_descent_multiple_iterations",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PASSED: test_gradient_descent_multiple_iterations()\n"
     ]
    }
   ],
   "source": [
    "def test_gradient_descent_multiple_iterations():\n",
    "    \"\"\" Testcase for gradient_descent() for multiple iterations \"\"\"\n",
    "    \n",
    "    X = np.array([[71.99, 57.95, 73.10, 63.45, 82.74, 18.05, 80.31, 3.76, 66.02, 42.84],\n",
    "                   [67.21, 1.41, 30.45, 10.01, 97.86, 6.09, 52.75, 19.03, 80.13, 48.92]])\n",
    "    Y = np.array([0, 0, 0, 1, 0, 1, 0, 1, 0, 1])\n",
    "    num_iterations = 3\n",
    "    learning_rate = 0.001182\n",
    "    \n",
    "    w_expected = np.array([[-0.01149753],\n",
    "                           [-0.00984429]])\n",
    "    b_expected = 0.00029081894\n",
    "    \n",
    "    costs_expected = [0.6931471, 0.5818799, 0.5573549]\n",
    "\n",
    "    w, b, costs = gradient_descent(X, Y, num_iterations, learning_rate, print_costs=False)\n",
    "\n",
    "    assert isinstance(w, np.ndarray), 'Expected a Numpy array for w but got {0}'.format(type(w))\n",
    "    assert w.shape == w_expected.shape, 'Unexpected shape for w. Expected {0} but got {1}'.format(\n",
    "        w_expected.shape, w.shape)\n",
    "    assert np.allclose(w, w_expected), 'expected w to be {0}, but got {1}'.format(w_expected, w)\n",
    "\n",
    "    assert isinstance(b, float), 'Expected a float for b but got {0}'.format(type(b))\n",
    "    assert np.allclose(b, b_expected), 'expected b to be {0}, but got {1}'.format(b_expected, b)\n",
    "    \n",
    "    assert isinstance(costs, list), 'Expected a Python list for costs but got {0}'.format(type(costs))\n",
    "    assert len(costs) == len(costs_expected), 'Unexpected length for costs. Expected {0} but got {1}'.format(\n",
    "        len(costs_expected), len(costs))\n",
    "    assert np.allclose(costs, costs_expected), 'expected costs to be {0}, but got {1}'.format(costs_expected, costs)\n",
    "\n",
    "\n",
    "    print('PASSED: test_gradient_descent_multiple_iterations()')\n",
    "\n",
    "        \n",
    "# Run the test\n",
    "test_gradient_descent_multiple_iterations()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 Applying the Model\n",
    "\n",
    "You have now successfully written code to train a Logistic Regression model! Let's try it out! Although your model can support data with an arbitrary number of features $n$, we will work with a contrived problem (but a very real application of Deep Learning) that only has two features to keep things simple and easier to visualize. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 The Application\n",
    "\n",
    "Suppose you are an engineer at Apple working on designing the [processor](https://en.wikipedia.org/wiki/Apple-designed_processors) that will be going into the next generation flagship iPhones. Once the chips have been manufactured they are tested for defects, and defective chips are removed from being used in building iPhones. (If you want to learn more about this topic you can take ELEC402 and/or 502!)\n",
    "\n",
    "Now let's say that you want to predict the likelihood that a chip is defective based on two simple tests that each yield a score of 0 to 100. Lets say you have historical data from previous generations of Apple processors, and you want to use this to train a Logistic Regression model for predicting whether chips in future generations will be defective*. Specifically, for previous generations, you have data on how those chips scored on the two tests, and whether they were truly defective as determined by a series of other sophisticated tests.\n",
    "\n",
    "In terms of Logistic Regression, this means each sample of your training data, $x^{(i)}$, has two features ($n=2$) and each feature can have a value from 0-100. Each sample of your training data has a label, $y$, of 1 or 0 where 1 means \"is defective\" and 0 means \"is not defective\".\n",
    "\n",
    "\\* You're assuming this historical data has the same distribution as future data which probably isn't true in the case of modern processor design, but let's pretend.  We will discuss the importance of your data distribution in future lectures."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Load Data\n",
    "\n",
    "First, let's load the training data that we have prepared for you. The *a1_tools* package is a package we have written to provide some useful functions for this assignment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from a1_tools import load_defect_data\n",
    "X, Y = load_defect_data()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Visualizing Data\n",
    "\n",
    "When planning your Deep Learning strategy, you should get familiar with your data before you build anything. In practice, you should follow this advice. However, since this assignment is contrived and we knew things would work out, it's ok that we did the opposite by building our Logistic Regression implementation first. \n",
    "\n",
    "Let's look at the shape of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X.shape)\n",
    "print(Y.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that there are two features and 100 samples.  Now let's check the range of values that each feature can have."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Range of feature 0: {0} to {1}'.format(np.min(X[0,:]), np.max(X[0,:])))\n",
    "print('Range of feature 1: {0} to {1}'.format(np.min(X[1,:]), np.max(X[1,:])))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now let's look at a few actual samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(10):\n",
    "    print('Sample {0}: x: {1} y: {2}'.format(i, X[:,i], Y[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When possible, graphically visualizing your data can often give you further insight into how to plan your Deep Learning strategy. We will use matplotlib package now to generate a scatter plot of our data. Since there are only two features, we can generate a 2D scatter plot where each feature is represented on an axis, and the color of the scatter point will denote the class (0 or 1 i.e. not defective, defective) to which the data point belongs.\n",
    "\n",
    "**Notes**: The line *%matplotlib inline* is a a Jupyter notebook directive and called a [magic function](https://ipython.readthedocs.io/en/stable/interactive/magics.html#magic-matplotlib).  All you need to know is that this makes matplotlib show plots as images in your Jupyter notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as mpatches\n",
    "%matplotlib inline\n",
    "\n",
    "cmap = plt.cm.Spectral\n",
    "plt.scatter(X[0,:], X[1,:], c=Y, cmap=cmap)\n",
    "plt.xlabel('test 1')\n",
    "plt.ylabel('test 2')\n",
    "\n",
    "plt.legend(handles=[\n",
    "    mpatches.Patch(color=cmap(0), label='Not Defective'),\n",
    "    mpatches.Patch(color=cmap(cmap.N), label='Defective')\n",
    "])\n",
    "\n",
    "plt.title(\"Chip Defects\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above figure, we can see that the data appears (more or less) linearly separable, meaning that, in the case of $n=2$, we can find a straight line that effectively separates the two classes. Recall that Logistic Regression finds linear partitions of the feature space.  Since the data looks linearly separable, this means that the problem is suitable for Logistic Regression. If the data were not linearly separable, you would not expect Logistic Regression to produce an accurate model, and therefore, you would know to try other machine learning techniques instead. That is why it is important to understand your data before building anything. Later in this assignment, we will look at data that is not linearly separable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 Training the Model (Running Gradient Descent)\n",
    "\n",
    "Finally, let's run Gradient Descent and train our model!  Run the code below which executes Gradient Descent. We have picked a learning rate that works well for this problem.  We will discuss how to choose the learning rate in a future lecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-21-e02f1df6aa9e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcosts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgradient_descent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m80000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.001182\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# Expected final cost: 0.22623328858976322\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Final cost: {0}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcosts\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'X' is not defined"
     ]
    }
   ],
   "source": [
    "w, b, costs = gradient_descent(X, Y, 80000, 0.001182)\n",
    "\n",
    "# Expected final cost: 0.22623328858976322\n",
    "print('Final cost: {0}'.format(costs[-1]))\n",
    "print()\n",
    "\n",
    "# Expected output: \n",
    "# [[-0.05769606]\n",
    "#  [-0.06118479]]\n",
    "print(w)\n",
    "print()\n",
    "\n",
    "# Expected output: 5.410521971317577\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.5 Visualize The Cost During Training\n",
    "\n",
    "For interest sakes, let's see how the cost changed as Gradient Descent progressed. In practice, you want to keep an eye on this as training takes place (which could be hours to days to weeks) to ensure that the cost is actually going down."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(costs)\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('Cost')\n",
    "plt.title(\"Logistic Regression Training Cost Progression\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.6 Making predictions with the model\n",
    "\n",
    "Now that you have trained the model, you can use it to make predictions! It's as simple computing the hypothesis function $A = \\sigma(w^T X + b)$ using our trained parameters, and new data.  And we already wrote this Python function in Section 2.3!\n",
    "\n",
    "There is one more thing to do though. Recall that although we use Logistic Regression for binary classification, the model still outputs a continuous value **between** 0 and 1. When using the model for prediction, we need to round its output 0 or 1.\n",
    "\n",
    "Complete the following code to build a Python function to perform predictions using the trained model parameters.\n",
    "\n",
    "**Hint**: You may want to use the function [np.rint()](https://docs.scipy.org/doc/numpy/reference/generated/numpy.rint.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "4e0fc67ecb4f62ab9787e9b7b559e585",
     "grade": false,
     "grade_id": "predict",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: predict\n",
    "\n",
    "def predict(X, w, b):\n",
    "    \"\"\" Use the Logistic Regression Model to make predictions\n",
    "    \n",
    "    Inputs:\n",
    "        X: NumPy array (n, m) of feature data\n",
    "        w: NumPy array (n, 1) of trained parameters w\n",
    "        b: float for trained bias parameter\n",
    "    \n",
    "    Returns:\n",
    "        NumPy array (m, ) of predictions.  Values are 0 or 1.\n",
    "    \"\"\"\n",
    "    y_pred = None\n",
    "\n",
    "    ### START CODE HERE ### (~1  line of code)\n",
    "    y_pred = np.rint(hypothesis(X,w,b))\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "708dc3f8af164f8f179e002c255af59e",
     "grade": true,
     "grade_id": "test_predict",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PASSED: test_predict()\n"
     ]
    }
   ],
   "source": [
    "def test_predict():\n",
    "    \"\"\" Testcase for predict() \"\"\"\n",
    "    \n",
    "    X = np.array([[59.30, 61.80, 77.68,  9.35],\n",
    "                  [72.73, 68.96, 17.92, 35.19]])\n",
    "    \n",
    "    w = np.array([[-0.05769606],\n",
    "                  [-0.06118479]])\n",
    "    b = 5.41052197\n",
    "     \n",
    "    Y_expected = np.array([[0, 0, 0, 1]])\n",
    "    Y = predict(X, w, b)\n",
    "\n",
    "    assert isinstance(Y, np.ndarray), 'Expected a Numpy array for Y but got {0}'.format(type(Y))\n",
    "    assert Y.shape == Y_expected.shape, 'Unexpected shape for Y. Expected {0} but got {1}'.format(\n",
    "        Y_expected.shape, Y.shape)\n",
    "    assert np.allclose(Y, Y_expected), 'expected Y to be {0}, but got {1}'.format(Y_expected, Y)\n",
    "\n",
    "    \n",
    "\n",
    "    print('PASSED: test_predict()')\n",
    "\n",
    "        \n",
    "# Run the test\n",
    "test_predict()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Congratulations! You have successfully written code to train a Logistic Regression model, and used the model to predict whether a chip is defective! You're done... no wait!\n",
    "\n",
    "**Wait!**\n",
    "\n",
    "Before you run off to your manager to say that you are now ready to predict which chips should be thrown in the trash, how confident are you in your model? I mean, if your model isn't very accurate, you'll end up costing the company lots of money by throwing away good chips, or worse yet, let a bunch of bad chips end up in iPhones that reach the hand of the users.  And boy, that would be a bad look for Apple! Here is a [case](https://en.wikipedia.org/wiki/IPhone_8#Issues) where iPhones with defective logic boards made it market.\n",
    "\n",
    "We need to assess the quality of our trained model.  In real Deep Learning applications, this is just the starting point.  Much of developing Deep Learning applications is in iteratively assessing your model, and finding ways to improve it. This is a big topic which we will look into during the second half of the course. For now, let's look at a couple simple ways to assess the quality of the model you just trained."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.7 Visualizing the Decision boundary\n",
    "\n",
    "Logisitic Regression is basically determining a line (for data with 2 features), as defined by the learned parameters, that separates the feature space into two partitions. The model will then hypothesize that all points in one partition (i.e. on one side of the line) will fall into the same class (e.g. defective). How *good* of a decision boundary does our trained model produce?\n",
    "\n",
    "Mathematically, the decision boundary is the set of points in the feature space where the model predicts a value of 0.5.  Therefore, the line for the decision boundary is simply the line that satisfies:\n",
    "\n",
    "$$ \\sigma(w_0x_0 + w_1x_1 + b) = 0.5 $$\n",
    "\n",
    "Convince yourself that rearranging the above to solve for $x_1$ yields:\n",
    "\n",
    "$$ x_1 = -\\frac{w_0x_0 + b}{w_1} $$\n",
    "\n",
    "We have written the function *plot_decision_boundary* for you and you can simply call it below. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "793a4420c881f8de0014193998600683",
     "grade": false,
     "grade_id": "cell-41595f0094dfe851",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def plot_decision_boundary(X, Y, w, b):\n",
    "    \"\"\" Plot decision boundary for 2D data\n",
    "    \n",
    "    Inputs:\n",
    "        X: NumPy array (2, m) of feature data\n",
    "        Y: NumPy array (m,)   of label data\n",
    "        w: NumPy array (n, 1) of trained parameters w\n",
    "        b: float for the bias parameter\n",
    "    \"\"\"\n",
    "    # Plot the original data\n",
    "    plt.scatter(X[0,:], X[1,:], c=Y, cmap=plt.cm.Spectral)\n",
    "    \n",
    "    # Plot the decision boundary line\n",
    "    # Pick the extremes of x_0 (and go beyond a little bit)\n",
    "    # Compute x_1 for these values according to the decision\n",
    "    # boundary equation, and plot the line\n",
    "    x_0 = np.array([min(X[0,:]) - 1, max(X[0,:]) + 1])\n",
    "    x_1 = - (w[0] * x_0 + b) / w[1]\n",
    "    plt.plot(x_0, x_1, label = \"Decision_Boundary\")\n",
    "\n",
    "    # scale the plot and other formatting\n",
    "    axes = plt.gca()\n",
    "    axes.set_xlim([min(X[0,:]),max(X[0,:])])\n",
    "    axes.set_ylim([min(X[1,:]),max(X[1,:])])\n",
    "    plt.ylabel('test 1')\n",
    "    plt.xlabel('test 2')\n",
    "    plt.show()\n",
    "    \n",
    "plot_decision_boundary(X, Y, w, b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hopefully you can see that the model does a pretty good job of putting all the red dots on one side of the line and blue dots on the other side. There are some dots that are on the \"wrong\" side, but if you look closely, you will see that there is no linear line that can perfectly separate the two classes.  This is often the case in practice for various reasons such as noise in the data, or a relationship that is not perfectly captured by the chosen features, or a relationship that is not perfectly linearly separable, etc.\n",
    "\n",
    "We have now qualitatively checked that our trained model seems to do pretty well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.8 Measuring Accuracy\n",
    "\n",
    "Let's also look at how we can quantitatively assess our trained model. One common metric is accuracy.  Given a set of feature data $X$ with known labels $Y$, how accurate is the model's predictions? Can our model achieve 100% accuracy?\n",
    "\n",
    "$$ accuracy = \\frac{correct\\_predictions}{all\\_predictions} $$\n",
    "\n",
    "Complete the following code to calculate the accuracy of predictions using a vectorized implementation (don't use loops). Specifically:\n",
    "\n",
    "- Make predictions on $X$ using the trained model to get $Y_{predicted}$\n",
    "- Compare $Y_{predicted}$ to the actual known labels $Y$ and compute the $accuracy$\n",
    "\n",
    "\n",
    "**Hints**: There are a number of ways to do this but here are some possibly useful tips\n",
    "\n",
    "- Boolean operations work on NumPy arrays.  For example, if $A$ and $B$ are arrays, $A == B$ will perform the element-wise boolean comparison\n",
    "- In Python and NumPy, performing arithmetic operations on boolean values (True/False) will treat the value True as numeric 1 and the value False as numeric 0.  Therefore, you may want to use [np.sum()](https://docs.scipy.org/doc/numpy/reference/generated/numpy.sum.html) and/or [np.mean()](https://docs.scipy.org/doc/numpy/reference/generated/numpy.mean.html) to help compute accuracy. You could also cast the Booleans to floats first if you want."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "1781ab451e33876c75e25772e969a669",
     "grade": false,
     "grade_id": "compute_accuracy",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: compute_accuracy\n",
    "\n",
    "def compute_accuracy(X, Y, w, b):\n",
    "    \"\"\" Compute the accuracy of a trained Logistic Regression \n",
    "        model described by its trained parameters\n",
    "\n",
    "    Inputs:\n",
    "        X: NumPy array (n, m) feature data\n",
    "        Y: NumPy array (m, ) known labels for feature data X\n",
    "        w: NumPy array (n, 1) trained model parameters w\n",
    "        b: float for trained bias parameter\n",
    "    \n",
    "    Returns:\n",
    "        float between 0 and 1 denoting the accuracy of the\n",
    "        Logistic Regression model \n",
    "    \"\"\"\n",
    "    accuracy = None\n",
    "    \n",
    "    ### START CODE HERE ### (1-2  line of code)\n",
    "    accuracy = np.mean(Y == predict(X,w,b))\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    return accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "9cf63ea9310a2b4a3f886c168eba3a1e",
     "grade": true,
     "grade_id": "test_compute_accuracy",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PASSED: test_compute_accuracy()\n"
     ]
    }
   ],
   "source": [
    "def test_compute_accuracy():\n",
    "    \"\"\" Testcase for compute_accuracy() \"\"\"\n",
    "    \n",
    "    X = np.array([[59.30, 61.80, 77.68,  9.35],\n",
    "                  [72.73, 68.96, 17.92, 35.19]])\n",
    "    \n",
    "    w = np.array([[-0.05769606],\n",
    "                  [-0.06118479]])\n",
    "    b = 5.41052197 \n",
    "    Y = np.array([[0, 1, 0, 1]])\n",
    "    \n",
    "    accuracy_expected = 0.75\n",
    "    \n",
    "    accuracy = compute_accuracy(X, Y, w, b)\n",
    "\n",
    "    assert isinstance(accuracy, float), 'Expected a float for accuracy but got {0}'.format(type(accuracy))\n",
    "    assert np.allclose(accuracy, accuracy_expected), 'expected db to be {0}, but got {1}'.format(accuracy_expected, accuracy)\n",
    "    \n",
    "\n",
    "    print('PASSED: test_compute_accuracy()')\n",
    "\n",
    "        \n",
    "# Run the test\n",
    "test_compute_accuracy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's compute the accuracy of our model using the same data that we used for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = compute_accuracy(X, Y, w, b)\n",
    "print('Accuracy: {0}'.format(accuracy))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "93% accuracy sounds pretty good!  At this point, it is easy to fool ourselves into thinking that our model is very accurate.  But all we know is that the model is accurate on the data on which it was trained. If you think about it, the model was optimized using this data, so we sure would hope that the model does making predictions on it! But how well does our model generalize to data that it hasn't seen before?  We will talk about this topic more later in the course and in future assignments. But not to be disheartened, having high accuracy on your training data is almost always a pre-requesite to achieving high accuracy on new data!  \n",
    "\n",
    "There are other metrics aside from accuracy.  For example sensitivity and specificity. Perhaps in this scenario, you care a lot about false positives - i.e. you dont want a good chip being classified as bad and lose money that way.  Or maybe you actually care more about false negatives meaning you are more worried about bad chips being deemed as good and making its way to market.  Thinking about the right metric to monitor is important and we will talk about this more later in class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4 Try different Hyperparameters\n",
    "\n",
    "We've heard briefly in class that the number of training iterations, and learning rate are called Hyperparameters.  Hyperparameters are another set of parameters (so as not to be confused with learned parameters of the model itself) that we will ultimately need to tune. In other words, how do we choose suitable values?  We will cover this topic in the second half of the course.  For now, we have picked values that work well for this assignment and data set.\n",
    "\n",
    "As an exercise, let's try changing the number of iterations and learning rate and see what happens.  For convenience, we have defined the following function *train_and_assess()* which collects the pertinent code you wrote earlier. You can simply use it in the following sections.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_assess(X, Y, num_iterations=80000, learning_rate=0.001182):\n",
    "    \"\"\" Trains a Logistic Regression model, then access its accuracy\n",
    "        and decision boundary on the training data.\n",
    "    \n",
    "    Inputs:\n",
    "        X: NumPy array (n, m) feature data\n",
    "        Y: NumPy array (m, ) known labels for feature data X\n",
    "        num_iterations: int for number of gradient descent iterations\n",
    "        learning_rate: float for gradient descent learning rate\n",
    "    \"\"\"\n",
    "    \n",
    "    # Train model\n",
    "    w, b, costs = gradient_descent(X, Y, num_iterations, learning_rate)\n",
    "    print('Final Cost: {0}'.format(costs[-1]))\n",
    "\n",
    "    # Plot Cost during training\n",
    "    plt.plot(costs)\n",
    "    plt.xlabel('Iteration')\n",
    "    plt.ylabel('Cost')\n",
    "    plt.title(\"Logistic Regression Training Cost Progression\")\n",
    "    plt.show()\n",
    "\n",
    "    # Plot decision boundary \n",
    "    plot_decision_boundary(X, Y, w, b)\n",
    "\n",
    "    # Print accuracy\n",
    "    accuracy = compute_accuracy(X, Y, w, b)\n",
    "    print('Accuracy: {0}'.format(accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. 1 Different Number of Training Iterations\n",
    "\n",
    "Leave the learning rate at 0.001182 and try different values of training iterations. See how this affects the decision boundary and accuracy.  Suggested values to try are 1000, 10000, 300000. Or try your own value! You should see that the longer the training goes, the better the fit, and hence accuracy.  While this is generally true for training data accuracy, we will see in the future that this doesn't mean that the model will become more accurate on data that it has never seen before.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_and_assess(X, Y, num_iterations=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_and_assess(X, Y, num_iterations=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_and_assess(X, Y, num_iterations=300000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Different Learning Rate\n",
    "\n",
    "The learning rate dictates the rate of change in the parameters on each iteration of training. \n",
    "\n",
    "#### 4.2.1 Learning Rate too small\n",
    "\n",
    "Try a smaller training rate like 0.0001.  What you should see is that the cost reduces at a much slower rate.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_and_assess(X, Y, learning_rate=0.0001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.2.2 Learning Rate too big\n",
    "\n",
    "Now try a larger learning rate of 0.002, but set the number of iterations to 100.  We choose a smaller number of iterations just so we can get a clearer plot of the change in cost.  You should see the cost oscillating but still trending downwards.  The oscillating behaviour also slows down the rate at which the cost decreases.  Do you remember from lecture why this oscillating behaviour occurs?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_and_assess(X, Y, num_iterations=100, learning_rate=0.002)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Data that is not linearly separable\n",
    "\n",
    "In this assignment, you have built a Logistic Regression training and prediction system!  We have learned that Logistic Regression works well for data that is linearly separable. However, many problems exist where this isn't the case. In other words, the decision boundary in those cases isn't a straight line (for 2D data).  This is motivation for using Neural Networks (Assignment 2) which has the power to learn non-linear decision boundaries.  And furthermore, Deep Neural Networks (Assignment 3) have the power to learn even more complex non-linearities.  \n",
    "\n",
    "To convince you, we have prepared a few data sets that are not linearly separable.  Try training your Logistic Regression model on them and see what kind of accuracy you can get.  You should see that when restricted to a linear decision boundary, you cannot get high accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from a1_tools import load_swirls, load_noisy_circles, load_noisy_moons, load_partitioned_circles\n",
    "\n",
    "# Uncomment only one of these lines to load the desired data set\n",
    "X, Y = load_swirls()\n",
    "# X, Y = load_noisy_circles()\n",
    "# X, Y = load_noisy_moons()\n",
    "# X, Y = load_partitioned_circles()\n",
    "\n",
    "\n",
    "# Plot the data\n",
    "plt.scatter(X[0,:], X[1,:], c=Y, cmap=plt.cm.Spectral)\n",
    "plt.show()\n",
    "\n",
    "# Now train and assess.  NOTE: The default learning rate and number of\n",
    "# training iterations were tuned for our linearly separable data set\n",
    "# you may want to play around with those numbers.\n",
    "train_and_assess(X, Y, num_iterations=10000, learning_rate = 0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Conclusion\n",
    "\n",
    "Congratulations on completing your first assignment! It was a long one, but we have covered a lot of material that will let us move faster in the following assignments.\n",
    "\n",
    "In this assignment, you have:\n",
    "\n",
    "- Learned to use various practical tools used in Deep Learning implementations such as Python, Jupyter Notebooks, NumPy, matplotlib.\n",
    "- Learned to use NumPy to work with arrays, use its various functions, and uses its various mechanisms such as vectorization and Broadcasting.\n",
    "- Built a complete Logistic Regression system that that can be used to train on data of any number of features!\n",
    "- Explored some of the considerations when training (e.g. choosing hyperparameters)\n",
    "- Explored qualitative and quantitative techniques for assessing the quality of your trained model\n",
    "\n",
    "In the next assignment, you will follow the same framework to build your first Neural Network!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thank you to Singulos Research for feedback, suggestions, and testing of this assignment."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
